{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series forecasting\n",
    "\n",
    "Time series forecasting is one of the most common problems one encounters in the machine learning domain. You can find time series forecasting problems everywhere, whether its predicting the stock market prices or predicting sales for a particular item on an ecommerce website or predicting readings of an IoT device placed in a factory. Timeseries forecasting has be approached using quite a few methods. You can find a brief of the different approaches here.\n",
    "\n",
    "We will be using Amazon SageMaker's builtin DeepAR algorithm to perform this task. DeepAR is an algorithm that uses Recurrent Neural Networks to forecast univariate time series. There are various aspects of DeepAR that are unique and you can find out more about how DeepAR differs from other algorithms here.\n",
    "\n",
    "The dataset we are using for this is [this one](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption). The dataset has 2 million readings of household electricity consumption in Sceaux between 2006 and 2010. We intend to train a model that will effectively and accurately predict the electricity consumption in various rooms within the household."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook was created and tested on an ml.m4.2xlarge_\n",
    "- You will require an Amazon S3 bucket to store the data in. This has to be in the same region as your Amazon SageMaker instance. [Here](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) are the details on how you can create an Amazon S3 bucket.\n",
    "- An IAM role ARN to provide training and hosting access to your data. You can find further details in the documentation on how to create these roles [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to download the data from the UCI website. [Here](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption) are the details of the dataset.\n",
    "You can either download this data to the notebook instance or to an Amazon S3 bucket of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from dateutil.parser import parse\n",
    "from sagemaker import get_execution_role,Session,estimator,predictor ## install this with 'sudo pip install sagemaker'\n",
    "\n",
    "roleARN = get_execution_role()\n",
    "bucket = <your-s3-bucket-name> ## Replace with your bucket name\n",
    "prefix = 'sagemaker/data/Household_Electricity_Consumption' ## Replace with the folder structure inside your bucket or simply ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import bisect\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sagemaker.amazon.common as smac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('household_power_consumption.zip') as zfl:\n",
    "    zfl.extractall()\n",
    "    print(\"Following files have been extracted: \\n{}\".format(zfl.namelist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a first look at the data. Notice the number of data fields, there is also a relationship between the fields, you can examine the relationship as explained on the webssite where the dataset is hosted. Also, observe that the scales for the fields are very different, so, for example, observe that Global_active_power and Sub_metering_1 have very different scales. Noice also that we haven't normalized the data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head household_power_consumption.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l household_power_consumption.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the dataframe and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata = pd.read_csv('./household_power_consumption.txt',sep=';',header=0,index_col=[0],parse_dates=[0,1],dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing quite a few things here. Notice from the previous output when examining the dataframe we just created above, the data has a granularity of 60 seconds. While we can use this for time series forecasting, we are not able to consider quite a few factors that we would be able to if the data had a larger frequency. With this in mind we are changing the data sampling from a minute to an hour. We are also replacing missing values with NaNs to avoid bias, you can do this with DeepAR, you can read all about DeepARs support for missing values [here](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-deepar-now-supports-missing-values-categorical-and-time-series-features-and-generalized-frequencies/). \n",
    "\n",
    "We are also creating a few new columns that we have added to the dataframe to ease our calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata.replace('?',np.nan,inplace=True)\n",
    "inputdata[['Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']] = inputdata[['Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']].apply(pd.to_numeric)\n",
    "inputdata['Date2'] = inputdata.index.values\n",
    "inputdata['TimeHrs'] = inputdata['Time'].dt.hour\n",
    "inp = inputdata\n",
    "grp = inp.groupby(['Date2','TimeHrs'],as_index=False)\n",
    "inp1 = grp['Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3'].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inp1.head(48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepAR supports Dynamic Features, these are crucial in training a model that considers factors that may affect our predictions but are outside of the data that we have. In this case, we are considering solstices within a year to make predictions in terms of how the weather might be affecting electricity consumption. For example, sub-meters serving certain appliances, like air conditioning will observe particularly high consumption in the hot summer months i.e. if the date ranges we are predicting for are closer to the summer solstice. We used the [pyEphem package](https://pypi.org/project/pyephem/) to build an array of yearwise (_for the years under consideration_) dates for solstices (_as they can differe slightly from year to year_).\n",
    "\n",
    "Since this is a one time exercise that need not be repeated everytime this notebook is created, we have added the list of solstice dates as an array below and have also added the code that you can use to generate this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sollistv2 = [datetime.date(2000, 6, 21), datetime.date(2000, 12, 21), datetime.date(2001, 6, 21), datetime.date(2001, 12, 21), datetime.date(2002, 6, 21), datetime.date(2002, 12, 22), datetime.date(2003, 6, 21), datetime.date(2003, 12, 22), datetime.date(2004, 6, 21), datetime.date(2004, 12, 21), datetime.date(2005, 6, 21), datetime.date(2005, 12, 21), datetime.date(2006, 6, 21), datetime.date(2006, 12, 22), datetime.date(2007, 6, 21), datetime.date(2007, 12, 22), datetime.date(2008, 6, 20), datetime.date(2008, 12, 21), datetime.date(2009, 6, 21), datetime.date(2009, 12, 21), datetime.date(2010, 6, 21), datetime.date(2010, 12, 21), datetime.date(2011, 6, 21), datetime.date(2011, 12, 22), datetime.date(2012, 6, 20), datetime.date(2012, 12, 21), datetime.date(2013, 6, 21), datetime.date(2013, 12, 21), datetime.date(2014, 6, 21), datetime.date(2014, 12, 21), datetime.date(2015, 6, 21), datetime.date(2015, 12, 22), datetime.date(2016, 6, 20), datetime.date(2016, 12, 21), datetime.date(2017, 6, 21), datetime.date(2017, 12, 21), datetime.date(2018, 6, 21), datetime.date(2018, 12, 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build the _sollistv2_ array like its done here below, you could use this code in a file in the terminal on this Jupyter Notebook instance. You can then use the array for the code in the next cell. You would need to install pyephem using pip prior to running this program. Although for the purposes for this notebook, you don't have to run this again as the data output by this code will remain the same and has been assigned to the _sollistv2_ above.\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import ephem\n",
    "import datetime\n",
    "\n",
    "def main():\n",
    "        sollist,sollistv2 = [],[]\n",
    "        for yr in range(2000,2019):\n",
    "                nsol1 = ephem.next_solstice(str(yr))\n",
    "                nsol2 = ephem.next_solstice(nsol1)\n",
    "                strdt = nsol1.datetime().strftime(\"%Y-%m-%d\")\n",
    "                sollist.append(strdt)\n",
    "                sollistv2.append(datetime.datetime.strptime(sollist[-1],\"%Y-%m-%d\").date())\n",
    "                strdt = nsol2.datetime().strftime(\"%Y-%m-%d\")\n",
    "                sollist.append(strdt)\n",
    "                sollistv2.append(datetime.datetime.strptime(sollist[-1],\"%Y-%m-%d\").date())\n",
    "        print(sollistv2)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "        main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dynamic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Features are optional input that you can provide as part of the training input to DeepAR. They have be an array of arrays of floats or integers that represent custom features. In this case, we are using the distance from the previous and next solstices for a given date. These, as described above, are going to be our custom features.\n",
    "\n",
    "We are populating values for these dynamic features. Note that while DeepAR can handle missing values in data series, dynamic features cannot have NaN/missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We calculate the distances from solstices and return the values, the input is observation date \n",
    "## i.e. same name as the column in our dataframes(inputs). We then populate the other columns.\n",
    "def getDistFromSolstices(obdt):\n",
    "    indx = bisect.bisect(sollistv2,obdt.date())\n",
    "    return (obdt.date() - sollistv2[indx-1]).days,(sollistv2[indx] - obdt.date()).days\n",
    "\n",
    "def populateDistance(data):\n",
    "    dsfromlast,dsfromnext = [],[]\n",
    "    for row in data.itertuples():\n",
    "        dfromlast,dfromnext = getDistFromSolstices(row[1]) #Get 'Date2' column\n",
    "        dsfromlast.append(dfromlast)\n",
    "        dsfromnext.append(dfromnext)\n",
    "    data = data.assign(distanceFromLastSolstice=pd.Series(dsfromlast,index=data.index))\n",
    "    data = data.assign(distanceFromNextSolstice=pd.Series(dsfromnext,index=data.index))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedDF = populateDistance(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processedDF.head(48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix all NaN values before we start writing this to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedDF.fillna(value=\"NaN\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features can be used to encode groups to which the record belongs. Here we group Global_active_power, Sub_metering_1, Sub_metering_2 and Sub_meter_3. We have created separate groups for Global_reactive_power and Voltage. The reason we use this kind of grouping is because the first group mentioned before have a relationship, ref. [dataset document here](https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be separating our test and training data, using the data from some of the years that we resampled previous. Note the categories and dynamic features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the training data \n",
    "\n",
    "trdata = []\n",
    "\n",
    "## Category for Global Active Power, Sub-Meter-1,2,3 -> 0\n",
    "starttime = str(processedDF.iloc[0]['Date2'])[:10] +' '+str(processedDF.iloc[0]['TimeHrs'])+':00:00'\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Global_active_power'].tolist()\n",
    "cat = [0,0]\n",
    "dynfeat = [processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['distanceFromLastSolstice'].tolist(),processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['distanceFromNextSolstice'].tolist()]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "## llly, for sub-meter-1\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Sub_metering_1'].tolist()\n",
    "cat = [0,1]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "## llly, for sub-meter-2\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Sub_metering_2'].tolist()\n",
    "cat = [0,2]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "## llly, for sub-meter-2\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Sub_metering_3'].tolist()\n",
    "cat = [0,3]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Global_reactive_power'].tolist()\n",
    "cat = [1,4]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Voltage'].tolist()\n",
    "cat = [2,5]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "random.shuffle(trdata)\n",
    "\n",
    "f = open(\"trainingdata.json\",'wb')\n",
    "for datapoint in trdata:\n",
    "    f.write(json.dumps(datapoint).encode(\"utf-8\"))\n",
    "    f.write(\"\\n\".encode('utf-8'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the test data\n",
    "\n",
    "tedata = []\n",
    "\n",
    "## Category for Global Active Power, Sub-Meter-1,2,3 -> 0\n",
    "\n",
    "starttime = str(processedDF[(processedDF['Date2'] == '2008-01')].iloc[0]['Date2'])[:10]+' '+ '0' + str(processedDF[processedDF['Date2'] == '2008-01'].iloc[0]['TimeHrs'])+':00:00'\n",
    "\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Global_active_power'].tolist()\n",
    "cat = [0,0]\n",
    "dynfeat = [processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['distanceFromLastSolstice'].tolist(),processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['distanceFromNextSolstice'].tolist()]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "## llly, for sub-meter-1\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Sub_metering_1'].tolist()\n",
    "cat = [0,1]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "## llly, for sub-meter-2\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Sub_metering_2'].tolist()\n",
    "cat = [0,2]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "## llly, for sub-meter-2\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Sub_metering_3'].tolist()\n",
    "cat = [0,3]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Global_reactive_power'].tolist()\n",
    "cat = [1,4]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Voltage'].tolist()\n",
    "cat = [2,5]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "f = open(\"testdata.json\",'wb')\n",
    "for datapoint in tedata:\n",
    "    f.write(json.dumps(datapoint).encode(\"utf-8\"))\n",
    "    f.write(\"\\n\".encode('utf-8'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training and testing files we created to our S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_key = 'trainingdata.json'\n",
    "testing_key = 'testdata.json'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_prefix = '{}/{}'.format(prefix,'test')\n",
    "\n",
    "## Lets create the sagemaker session and upload the data from where Amazon SageMaker will pick it \n",
    "## up to put in the container\n",
    "sg_sess = Session()\n",
    "training_path  = sg_sess.upload_data(training_key, bucket=bucket, key_prefix=train_prefix)\n",
    "testing_path = sg_sess.upload_data(testing_key, bucket=bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the DeepAR image,\n",
    "* Set the context length,\n",
    "* Set the prediction length,\n",
    "* Set the hyperparameters for the training job,\n",
    "* Launch the training job\n",
    "\n",
    "Finally, attach an estimator to the training job to check its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "img = get_image_uri(boto3.Session().region_name,'forecasting-deepar')\n",
    "prediction_length = 24 # hours ahead\n",
    "context_length = 216 # hours prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'DevDay-DeepARTraining-' + strftime(\"%Y-%m-%d-%H-%M-%S\",gmtime())\n",
    "print(job_name)\n",
    "                                \n",
    "## We choose a different output location to keep these separate from our old artifacts\n",
    "output_location = 's3://{}/{}/output'.format(bucket,prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": img,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": roleARN,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": output_location\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 4,\n",
    "        \"InstanceType\": \"ml.c4.8xlarge\",\n",
    "        \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"time_freq\": 'H', # hourly series\n",
    "        \"context_length\": str(context_length),\n",
    "        \"prediction_length\": str(prediction_length), # number of data points to predict\n",
    "        \"num_dynamic_feat\": \"auto\",\n",
    "        \"num_cells\": \"60\", \n",
    "        \"num_layers\": \"3\",\n",
    "        \"likelihood\": \"gaussian\", #real world data\n",
    "        \"epochs\": \"250\", ## keep small, to save on time!!! 250 seems to give good results though\n",
    "        \"mini_batch_size\": \"512\", ## Keep large, 128 gives good results\n",
    "        \"learning_rate\": \"0.01\",\n",
    "        \"dropout_rate\": \"0.1\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 60 * 240 # Give it four hours at best, could increase this for production scale\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": training_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"test\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": testing_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sgmaker = boto3.client('sagemaker')\n",
    "sgmaker.create_training_job(**create_training_params)\n",
    "\n",
    "## Lets check the status of the job to see if its complete, and lets wait until its done\n",
    "status = sgmaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "tor = sagemaker.estimator.Estimator.attach(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the model artifacts after training and stored them in the S3 location we specified when we created the training job. The steps to prepare for deployment are,\n",
    "* Create a model,\n",
    "* Create an endpoint configuration,\n",
    "* Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = job_name\n",
    "\n",
    "info = sgmaker.describe_training_job(TrainingJobName=job_name)\n",
    "modeldata = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "container = {\n",
    "    'Image': img,\n",
    "    'ModelDataUrl': modeldata\n",
    "}\n",
    "\n",
    "created_model = sgmaker.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = roleARN,\n",
    "    PrimaryContainer = container)\n",
    "\n",
    "print(created_model['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'DevDayEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "created_endpoint_config = sgmaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + created_endpoint_config['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'DevDayEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "created_endpoint = sgmaker.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(created_endpoint['EndpointArn'])\n",
    "\n",
    "resp = sgmaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "try:\n",
    "    sgmaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "finally:\n",
    "    resp = sgmaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Create endpoint ended with status: \" + status)\n",
    "\n",
    "    if status != 'InService':\n",
    "        message = sgmaker.describe_endpoint(EndpointName=endpoint_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that when querying the model we just trained for inference, we trained the model on multiple timeseries, and because the model has been trained on multiple timeseries we should be able to infer for more than one timeseries basically all of the timeseries we used for training. We will see below that we can use the same model to forecast multiple time series.\n",
    "\n",
    "But first, lets write some convenience functions, the first of these builds the JSON formatted request that is required by DeepAR. The next invokes the endpoint and receives and returns the response from the model we deployed. Finally, we plot the results to compare what we predicted and the ground truth, just to see how well or badly we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A few convenience functions\n",
    "\n",
    "def build_request(seriesstr):\n",
    "    '''This function builds a request for a specific time series'''\n",
    "    if processedDF[processedDF['Date2'] == '2008-01'].iloc[0]['TimeHrs'] < 10:\n",
    "        starttime = str(processedDF[(processedDF['Date2'] == '2008-08')].iloc[0]['Date2'])[:10]+' '+ '0' + str(processedDF[processedDF['Date2'] == '2008-08'].iloc[0]['TimeHrs'])+':00:00'\n",
    "    else:\n",
    "        starttime = str(processedDF[(processedDF['Date2'] == '2008-08')].iloc[0]['Date2'])[:10]+' '+ str(processedDF[processedDF['Date2'] == '2008-08'].iloc[0]['TimeHrs'])+':00:00'\n",
    "\n",
    "    target = processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-14')][seriesstr].tolist()\n",
    "    cat = [0,0]\n",
    "    dynfeat = [processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-15')]['distanceFromLastSolstice'].tolist(),processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-15')]['distanceFromNextSolstice'].tolist()]\n",
    "    instances = []\n",
    "\n",
    "    instances.append({\n",
    "        \"start\": starttime,\n",
    "        \"target\": target,\n",
    "        \"cat\": cat,\n",
    "        \"dynamic_feat\": dynfeat\n",
    "    })\n",
    "\n",
    "    request = {}\n",
    "    request['instances'] = instances\n",
    "    request['configuration'] = {}\n",
    "    request['configuration']['num_samples'] = 50\n",
    "    request['configuration']['output_types'] = ['mean']\n",
    "    return(json.dumps(request))\n",
    "\n",
    "def getInferences(epname,seriesstr):\n",
    "    '''Get inferences from the deployed model'''\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=build_request(seriesstr))\n",
    "    return(json.loads(response['Body'].read().decode()))   \n",
    "\n",
    "def getTime(row):\n",
    "    if row['TimeHrs'] < 10:\n",
    "        return str(row['Date2'])[:10] + ' 0' + str(row['TimeHrs']) + \":00:00\"\n",
    "    else:\n",
    "        return str(row['Date2'])[:10] + ' ' + str(row['TimeHrs']) + \":00:00\"\n",
    "\n",
    "def plotResults(processedDF,seriesstr,result):\n",
    "    '''Plot the results against the truth'''\n",
    "    pltFrame = processedDF.replace(to_replace=\"NaN\",value=np.nan,regex=True)\n",
    "    pltFrame['TimeFull'] = pltFrame.apply(lambda row: getTime(row),axis=1)\n",
    "    pltFrame['TimeFull'] = pd.to_datetime(pltFrame['TimeFull'])\n",
    "    pltFrame.set_index('TimeFull',inplace=True)\n",
    "    predictions = pd.Series(data=result['predictions'][0]['mean'],index=pltFrame.index[pltFrame['Date2'] >= '2008-08-14'][:24])\n",
    "    pltf = pd.concat([pltFrame,predictions],axis=1)\n",
    "    pltf[0].plot(kind='line',style='ko--',use_index=True,rot=30,ylim=[0.0,3.0],xlim=[pltf.index[pltf['Date2'] >= '2008-08-13'][0],pltf.index[pltf['Date2'] >= '2008-08-14'][24]],figsize=(20,10))\n",
    "    pltf[seriesstr].plot(kind='line',style='bo--',use_index=True,rot=30,ylim=[0.0,3.0],xlim=[pltf.index[pltf['Date2'] >= '2008-08-13'][0],pltf.index[pltf['Date2'] >= '2008-08-14'][24]],figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see if we can forecast Global active power and plot the results our model returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotResults(processedDF,\n",
    "            'Global_active_power',\n",
    "            getInferences(endpoint_name,'Global_active_power'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now see how the same model performs for Global intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotResults(processedDF,\n",
    "            'Global_intensity',\n",
    "            getInferences(endpoint_name,'Global_intensity'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgmaker.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
