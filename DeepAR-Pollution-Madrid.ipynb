{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries analysis on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to predict values in the series CO, NO2, NOx, O3, PM10, SO2, there are various stations and we would like to segregate the data by station. We are using the Madrid Air Quality dataset from Kaggle here: https://www.kaggle.com/decide-soluciones/air-quality-madrid#madrid.h5\n",
    "Our Notebook instance comes installed with wget, which you can use, if you have an account on Kaggle.\n",
    "You can issue the following shell commands from the same directory as your Notebook, to get the file from Kaggle.\n",
    "\n",
    "`wget --user=rahulns73 --ask-password https://www.kaggle.com/decide-soluciones/air-quality-madrid/downloads/air-quality-madrid.zip`\n",
    "\n",
    "You can then unzip the file using the unzip utility, thus,\n",
    "\n",
    "`unzip <path/output-file-name>`\n",
    "\n",
    "We chose to use the madrid_2001.csv file, you can choose another if you so desire. You may need to analyze the file before proceeding with the rest of this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the records from 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "from sagemaker import estimator\n",
    "\n",
    "# Get our role setup\n",
    "role = get_execution_role()\n",
    "bucket = 'rnszsdemo' ## Replace with your bucket name\n",
    "prefix = 'sagemaker/data/Madrid_Air_Quality' ## Replace with the folder structure inside your bucket or simply ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import all the python packages we think we might need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sagemaker.amazon.common as smac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the data, start with building a pandas dataframe\n",
    "data = pd.read_csv('s3://{}/{}/madrid_2001.csv'.format(bucket,prefix),parse_dates=True,index_col=0)\n",
    "\n",
    "# Copy the index column\n",
    "data['processedDate'] = data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data, what have we got??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with the missing values and create separate dataframes for each station. We find the stations, then create a separate dataframe for the readings from each of the stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stnlist = list(data['station'].unique())\n",
    "stnelem = [data.loc[data['station'] == stn] for stn in stnlist]\n",
    "stnelem[3].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like hourly data, but we also know that there are a bunch of NaNs in this data. Many of the columns contain NaNs but those aren't of much interest to us anyway, so we are going to forward fill those. Nevertheless, we don't need to do that when using DeepAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanstnelem = [elem.drop(columns=['BEN','EBE','MXY','NMHC','OXY','PXY','TCH','TOL']) for elem in stnelem]\n",
    "eachstn = [] ## Lets keep this final data from each station here.\n",
    "for clelem in cleanstnelem:\n",
    "    eachstn.append(clelem.fillna(method='ffill'))\n",
    "eachstn[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data preprocessed, lets see what it looks like, lest we need to work on it a little more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets plot this data for some of the stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachstn[0]['O_3'][:'2001-01-25'].plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets try to predict the various series, in our case O_3 monthly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = []\n",
    "testSet = []\n",
    "\n",
    "## Set up the start times for our series, this is 1 to 12\n",
    "startTimes = eachstn[0].groupby([eachstn[0].index.month]).first()['processedDate'].tolist()\n",
    "\n",
    "## Don't want to keep these undefined\n",
    "trainingSet.append([0])\n",
    "testSet.append([0])\n",
    "\n",
    "## Lets get all the training examples\n",
    "for month in range(1,9): ##choose months January to August\n",
    "    trainingSet.append(eachstn[0]['O_3'][:'2001-{}-25'.format(month)].tolist())\n",
    "    testSet.append(eachstn[0]['O_3']['2001-{}-25'.format(month):'2001-{}-28'.format(month)].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our input dataset and test set. Once we are done with this, we can run our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeData(fname,dataset):\n",
    "    f = open(fname,'w')\n",
    "    for i in range(1,len(trainingSet)):\n",
    "        line = \"\\\"start\\\":\\\"{}\\\",\\\"target\\\":{}\".format(str(startTimes[i-1]),dataset[i])\n",
    "        f.write('{'+line+'}\\n')\n",
    "    f.close()\n",
    "    \n",
    "writeData('trainingData.json',trainingSet)\n",
    "writeData('testData.json',testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the file we have generated for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat testData.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us upload the data to S3\n",
    "We will use Amazon SageMaker's own function to do that. You can take a look at it __[here](http://sagemaker.readthedocs.io/en/latest/session.html)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_key = 'trainingData.json'\n",
    "testing_key = 'testData.json'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "output_prefix = '{}/{}'.format(prefix, 'output')\n",
    "\n",
    "## Lets create the sagemaker session and upload the data from where Amazon SageMaker will pick it \n",
    "## up to put in the container\n",
    "sg_sess = Session()\n",
    "training_path  = sg_sess.upload_data(training_key, bucket=bucket, key_prefix=train_prefix)\n",
    "testing_path = sg_sess.upload_data(testing_key, bucket=bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets set up the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set up the containers we intend to use and then create an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {\n",
    "    'us-east-1': '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-east-2': '566113047672.dkr.ecr.us-east-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-west-2': '156387875391.dkr.ecr.us-west-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'eu-west-1': '224300973850.dkr.ecr.eu-west-1.amazonaws.com/forecasting-deepar:latest'\n",
    "}\n",
    "\n",
    "b3session = boto3.session.Session()\n",
    "myregion = b3session.region_name\n",
    "\n",
    "img = containers[myregion]\n",
    "\n",
    "estimator = estimator.Estimator(\n",
    "    sagemaker_session=sg_sess,\n",
    "    image_name=img,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.8xlarge',\n",
    "    base_job_name='Madrid-Air-Quality',\n",
    "    output_path='s3://{}/{}'.format(bucket,output_prefix)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Hyperparameters for DeepAR, here they are,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__time_freq__ : The granularity of the time series in the dataset. Required. Use time_freq to select appropriate date features and lags. The model supports the following basic frequencies. It also supports multiples of these basic frequencies. For example, 5min specifies a frequency of 5 minutes.\n",
    " - M for monthly\n",
    " - W for weekly\n",
    " - D for daily\n",
    " - H for hourly\n",
    " - min for every minute\n",
    "\n",
    "__prediction_length__ : The number of time-steps that the model is trained to predict, also called the forecast horizon. The trained model always generates forecasts with this length. It can't generate longer forecasts. The prediction_length is fixed when a model is trained and it cannot be changed later. \n",
    "\n",
    "__context_length__ : The number of time-points that the model gets to see before making the prediction. The value for this parameter should be about the same as the prediction_length. The model also receives lagged inputs from the target, so context_length can be much smaller than typical seasonalities. For example, a daily time series can have yearly seasonality. The model automatically includes a lag of one year, so the context length can be shorter than a year. The lag values that the model picks depend on the frequency of the time series. For example, lag values for daily frequency are previous week, 2 weeks, 3 weeks, 4 weeks, and year.\n",
    "\n",
    "__likelihood__ : The model generates a probabilistic forecast, and can provide quantiles of the distribution and return samples. Depending on your data, select an appropriate likelihood (noise model) that is used for uncertainty estimates. The following likelihoods can be selected:\n",
    "**gaussian** : Use for real-valued data\n",
    "**beta** : Use for real-valued targets between 0 and 1 incl.\n",
    "**negative-binomial** : Use for count data (non-negative integers).\n",
    "**student-T** : An alternative for real-valued data that works well for bursty data.\n",
    "**deterministic-L1** : A loss function that does not estimate uncertainty and only learns a point forecast.\n",
    "\n",
    "__epochs__ : The maximum number of passes over the training data. The optimal value depends on your data size and learning rate. See also early_stopping_patience. Typical values range from 10 to 1000.\n",
    "\n",
    "There are more...if you want to know more check __[here](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are some recommended best practices that would be good to consider when using DeepAR. You can find them __[here](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 72 # Three days ahead?!\n",
    "hyperparameters = {\n",
    "    \"time_freq\": 'H', # hourly series\n",
    "    \"context_length\": prediction_length,\n",
    "    \"prediction_length\": prediction_length, # number of data points to predict\n",
    "    \"num_cells\": \"40\", #default\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"50\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"0.01\",\n",
    "    \"dropout_rate\": \"0.05\"#,\n",
    "#    \"early_stopping_patience\": \"10\" # what is this ??\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets train this baby!! :-)\n",
    "Note that when you call `fit()`, there is no option but to see the training job name in the console, it will be preceeded with the string `INFO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\"train\": training_path, \"test\": testing_path}\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the name of the model we just trained to use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_level_api_model = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively, you could be more sophisticated about the training,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the low-level SDK for Python which provides the create_traning_job method which maps to the _CreateTrainingJob_ Amazon SageMaker API. You will also be required to define the training parameters that give you more finegrained control over the training job.\n",
    "You can also use a _Waiter_ to wait until the training job is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'MyDeepARTraining-' + strftime(\"%Y-%m-%d-%H-%M-%S\",gmtime())\n",
    "print(job_name)\n",
    "                                \n",
    "## We have already setup the containers above\n",
    "                                \n",
    "## We choose a different output location to keep these separate from our old artifacts\n",
    "output_location = 's3://{}/{}/output-low-level-training'.format(bucket,prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": img,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": output_location\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.c4.8xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"time_freq\": 'H', # hourly series\n",
    "        \"context_length\": str(prediction_length*3),\n",
    "        \"prediction_length\": str(prediction_length), # number of data points to predict\n",
    "        \"num_cells\": \"40\", #default\n",
    "        \"num_layers\": \"2\",\n",
    "        \"likelihood\": \"gaussian\",\n",
    "        \"epochs\": \"50\",\n",
    "        \"mini_batch_size\": \"64\",\n",
    "        \"learning_rate\": \"0.01\",\n",
    "        \"dropout_rate\": \"0.05\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 60 * 60\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": training_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"test\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": testing_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sgmaker = boto3.client('sagemaker')\n",
    "sgmaker.create_training_job(**create_training_params)\n",
    "\n",
    "## Lets check the status of the job to see if its complete, and lets wait until its done\n",
    "status = sgmaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "try:\n",
    "    sgmaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "finally:\n",
    "    status = sgmaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "    if status == 'Failed':\n",
    "        message = sgmaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the model to Amazon SageMaker hosting services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 steps to host a model in Amazon SageMaker,\n",
    " - Create a model in Amazon SageMaker\n",
    " - Create an endpoint configuration\n",
    " - Create an endpoint\n",
    "\n",
    "Using the high level API, all it takes is a simple call to _deploy_, like so,\n",
    "    \n",
    "    my_predictor = myalgo.deploy(initial_instance_count=2,instance_type='ml.m4.xlarge')\n",
    "\n",
    "Lets use the low level API and use the models we generated above before(see above) to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = job_name\n",
    "print(modelname)\n",
    "\n",
    "## Lets find out where our model artifacts have been stored, first for the model artifacts from the low level SDK\n",
    "info = sgmaker.describe_training_job(TrainingJobName=job_name)\n",
    "modeldata = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "primary_ctnr = {\n",
    "    'Image': img,\n",
    "    'ModelDataUrl': modeldata\n",
    "}\n",
    "\n",
    "## Create the actual model\n",
    "cmr = sgmaker.create_model(\n",
    "    ModelName = job_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_ctnr)\n",
    "\n",
    "## You should now be able to get the model ARN\n",
    "print(cmr['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, if you have created a model using the high-level SDK, you can create the model like so,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = h_level_api_model \n",
    "information = sgmaker.describe_training_job(TrainingJobName=model_name) \n",
    "\n",
    "## All of the rest is pretty much similar\n",
    "model_data = information['ModelArtifacts']['S3ModelArtifacts']\n",
    "primary_container = {\n",
    "    'Image': img,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "## Create the actual model\n",
    "cmrv2 = sgmaker.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "## You should now be able to get the model ARN\n",
    "print(cmrv2['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we have two models, we would like to send 50% of the traffic to each of the endpoints, lets create an endpoint configuration for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'MyDeepAREP-'+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sgmaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants = [{\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 2,\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"ModelName\": model_name, ## The highlevel model we trained first\n",
    "            \"VariantName\": model_name\n",
    "        },\n",
    "        {\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 2,\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"ModelName\": modelname, ## The lowlevel model we trained second\n",
    "            \"VariantName\": modelname            \n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Endpoint Config Arn: \"+create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'MyDeepAREndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sgmaker.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sgmaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "try:\n",
    "    sgmaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "finally:\n",
    "    resp = sgmaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Create endpoint ended with status: \" + status)\n",
    "\n",
    "    if status != 'InService':\n",
    "        message = sgmaker.describe_endpoint(EndpointName=endpoint_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application autoscaling can be configured using __[this](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-add-policy.html)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReqSet = []\n",
    "ValidationChkSet = [] \n",
    "\n",
    "## Lets create a helper function for the creation of the request\n",
    "def createRequest(fname,dataset):\n",
    "    f = open(fname,'w')\n",
    "    line1 = \"\\\"start\\\":\\\"{}\\\",\\\"target\\\":{}\".format(str(startTimes[8]),dataset[1])\n",
    "    # We want only 10 samples for this exercise\n",
    "    line2 = \"\\\"instances\\\": [{\"+line1+\"}],\\\"configuration\\\": {\\\"num_samples\\\": 50,\\\"output_types\\\": [\\\"mean\\\",\\\"quantiles\\\",\\\"samples\\\"],\\\"quantiles\\\": [\\\"0.1\\\",\\\"0.9\\\"]}\"\n",
    "    f.write('{'+line2+'}\\n')\n",
    "    f.close()\n",
    "\n",
    "## Don't want to keep these undefined\n",
    "ReqSet.append([0])\n",
    "ValidationChkSet.append([0])    \n",
    "\n",
    "## Lets get what we need for our request first, we will use month 9, remember we have trained the model on the first 8 months.\n",
    "ReqSet.append(eachstn[0]['O_3'][:'2001-{}-25'.format(9)].tolist()) # You may __not__ want to **hardcode** this\n",
    "\n",
    "## The below will be used to check how we are predicting\n",
    "ValidationChkSet.append(eachstn[0]['O_3']['2001-{}-25'.format(9):'2001-{}-28'.format(9)].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createRequest('RequestData.json',ReqSet)\n",
    "createRequest('ValidationData.json',ValidationChkSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how the request files have turned out, note we didn't really need validation data to be put into the request format, but its easier to look at smaller data than the larger request data that has more data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ValidationData.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do the prediction\n",
    "We obtained the endpoint name from above endpoint creation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.Session().client('sagemaker-runtime')\n",
    "## Create bytes for the payload\n",
    "with open(\"ValidationData.json\",\"rb\") as f:\n",
    "    reqcontents = f.read()\n",
    "payload = reqcontents\n",
    "\n",
    "## This comes from the previous call that gives us our endpoint\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,ContentType='application/json',Body=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we have the predictions! Lets look at what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see from the above many of the details, we are specifically interested in the `InvokedProductionVariant`. What we get is a `botocore.response.StreamingBody`, thats where your predictions are. You need to be able to read the `StreamingBody` to get to the predictions.\n",
    "The Stream that we get from the `response`(above) can be used only once as it doesn't survive across calls. We get the whole thing in one read call, not really a best practice. You may want to read fewer bytes at a time by passing the `read()` a bytes parameter.\n",
    "Storing the stream in `datamy` helps me access the data later, as and when I want, now we can proceed with choosing to display one of the forecasts.\n",
    "Getting string data. If you want the `StreamingBody`, you would have to call invoke_endpoint again!!! Of course! :-)\n",
    "since we have already read the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "my_json = (response['Body']).read()\n",
    "datamy = json.loads(my_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "q1 = 0.1\n",
    "q2 = 0.9\n",
    "\n",
    "y_data = datamy['predictions'][0]\n",
    "y_mean = y_data['mean']\n",
    "y_q1 = y_data['quantiles'][str(q1)]\n",
    "y_q2 = y_data['quantiles'][str(q2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since we now have everything, lets plot this to compare the predictions with real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,prediction_length) \n",
    "plt.gcf().clear()\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "meanlbl, = plt.plot(x,y_mean,'k--',label='mean')\n",
    "q1lbl, = plt.plot(x,y_q1,'g',label=q1)\n",
    "q2lbl, = plt.plot(x,y_q2,'b',label=q2)\n",
    "truth_data = eachstn[0]['O_3']['2001-{}-26'.format(9):'2001-{}-28'.format(9)].tolist()\n",
    "gtlbl, = plt.plot(x,truth_data,'m',label='truth')\n",
    "plt.legend(handles=[meanlbl,q1lbl,q2lbl,gtlbl])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obviously need more training or need to tune our hyperparameters better. That bring us to \n",
    "### Automated Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to this __[page](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-tuning.html)__ for a detailed explanation of how to tune DeepAR models, and this __[page](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)__ for a detailed explanation of Automatic Model Tuning in Amazon SageMaker.\n",
    "Some other tools that you can use for automatic model tuning in Python : __[Spearmint](https://github.com/JasperSnoek/spearmint)__, __[MOE](https://github.com/Yelp/MOE)__, __[HyperOpt](https://github.com/hyperopt/hyperopt)__ and __[SMAC](https://github.com/automl/SMAC3)__.\n",
    "We will use AMT to tune our model like so,\n",
    " - We create an estimator, just like we did when we wanted to create the model (see above)\n",
    " - Then we setup the hyperparameter ranges that AMT will will explore to get the optimal model\n",
    " - Setup the `tuner`\n",
    " - Then run tuner.fit()\n",
    " \n",
    " To analyze the results of the hyperparameter optimzation task, use __[this notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import estimator\n",
    "\n",
    "training_key = 'trainingData.json'\n",
    "testing_key = 'testData.json'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train-HPO')\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test-HPO')\n",
    "output_prefix = '{}/{}'.format(prefix, 'output-HPO')\n",
    "\n",
    "## Lets create the sagemaker session\n",
    "sg_sessV2 = Session()\n",
    "training_path  = sg_sess.upload_data(training_key, bucket=bucket, key_prefix=train_prefix)\n",
    "testing_path = sg_sess.upload_data(testing_key, bucket=bucket, key_prefix=test_prefix)\n",
    "\n",
    "estimatorHPO = estimator.Estimator(\n",
    "    sagemaker_session=sg_sessV2,\n",
    "    image_name=img,\n",
    "    role=role,\n",
    "    train_instance_count=2,\n",
    "    train_instance_type='ml.c4.8xlarge',\n",
    "    base_job_name='Madrid-Air-Quality-With-HPO',\n",
    "    output_path='s3://{}/{}'.format(bucket,output_prefix)\n",
    ")\n",
    "prediction_length = 72 # Three days ahead?!\n",
    "## Here we don't have the hyperparameters that we intend to tune\n",
    "hyperparameters = {\n",
    "    \"time_freq\": 'H', # hourly series\n",
    "    \"context_length\": prediction_length,\n",
    "    \"prediction_length\": prediction_length, # number of data points to predict\n",
    "    \"num_cells\": \"40\", #default\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"50\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"0.01\",\n",
    "    \"dropout_rate\": \"0.05\"#,\n",
    "#    \"early_stopping_patience\": \"10\" # stop if loss hasn't improved in 10 epochs\n",
    "}\n",
    "\n",
    "estimatorHPO.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the hyperparameter ranges\n",
    "The HPO algorithm used by Amazon SageMaker will explore the ranges specified. With continuous ranges, it can explore \n",
    "a lot more values than with integer ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {'learning_rate': ContinuousParameter(0.01, 0.2),'num_layers': IntegerParameter(2,5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric definitions\n",
    "If you are using a builtin algorithm, like we are here, you should __not__ provide the metric definition.\n",
    "At other times, like if you are using your own say, tensorflow script __OR__ your own framework, you __will__ definitely have to provide metric definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'train:final_loss'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimatorHPO,objective_metric_name,hyperparameter_ranges,max_jobs=1,max_parallel_jobs=1,objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `include_cls_metadata=False` to the `fit()` method is required as Amazon SageMaker HPO for DeepAR will not work without it. To get more detailed information on why this argument to `fit()` is required, check __[here](https://github.com/aws/sagemaker-python-sdk/blob/master/README.rst)__.\n",
    "To check the status of your Hyperparameter tuning job, you can use this notebook (or the code in it): In your notebook instance click the `SageMaker Examples` tab, go to the section on `Hyperparameter Tuning`, `Use` the notebook `HPO_Analyze_TuningJob_Results.ipynb`. In this notebook, replace the name of the Hyperparameter tuning job with the one you get from the `fit()` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': training_path, 'test': testing_path},include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to delete your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = boto3.client('sagemaker')\n",
    "response = cl.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
